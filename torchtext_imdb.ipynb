{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torchtext_imdb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI-mwSCvE7xT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init \n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F \n",
        "\n",
        "from torchtext import data \n",
        "from torchtext import vocab \n",
        "from torchtext import datasets \n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHViyPZOF426"
      },
      "source": [
        "# Hyper Params \n",
        "batch_size = 32\n",
        "output_size = 2 \n",
        "hidden_size = 256 \n",
        "embedding_length = 300 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIlRmxnMGGao",
        "outputId": "9bba3a97-5fde-4798-9866-2be0749208af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "'''\n",
        "Step 1 : テキストの読み込み\n",
        "Step 2 : テキストの分割とトークン化\n",
        "・ Field クラスが、tochvision でいう transforms のイメージ\n",
        "'''\n",
        "\n",
        "tokenize = lambda x: x.split()\n",
        "TEXT = data.Field(\n",
        "    sequential=True, # テキストが可変長の場合に True （パディングの作成対象になる）\n",
        "    tokenize=tokenize, # トークン化の方法を記載した関数\n",
        "    lower=True, # 大文字を小文字に変換する場合 True \n",
        "    include_lengths=True, # イテレータに含まれる 1 テキストとの単語数を表示（イテレータが長さも含めたタプルを返す）\n",
        "    batch_first=True, # Tensor の1次元めをバッチサイズの次元にする\n",
        "    fix_length=200 # 1文の中で難単語まで使用するかを指定\n",
        "    )\n",
        "\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# データのダウンロード\n",
        "train_dataset, test_dataset = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_dataset, val_dataset = train_dataset.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWNrFuv2Gzbo",
        "outputId": "634465cc-a2e9-4e3d-9cc1-70493a9adeb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17500, 7500, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNmzeXB4HeuA",
        "outputId": "f4f2073f-4071-446e-feaf-0ce4d7828468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "'''\n",
        "Step 3 : トークンのインデックス化\n",
        "Step 4 : 複数テキストのバッチ化\n",
        "\n",
        "TEXT.build_vocab : 辞書を作成\n",
        "TEXT.vocab.freqs : コーパス中の単語毎の出現回数を表示\n",
        "TEXT.vocab.itos : index から string（単語） の変換\n",
        "TEXT.vocab.stoi : 逆\n",
        "TEXT.vocab.vectors : 学習済み埋め込みベクトルの指定\n",
        "'''\n",
        "\n",
        "# 辞書を作成\n",
        "TEXT.build_vocab(\n",
        "    train_dataset,\n",
        "    min_freq=3, # 出現頻度の低い単語を省く\n",
        "    vectors=vocab.GloVe(name=\"6B\", dim=300) # 学習済み埋め込みベクトルを適用\n",
        "    )\n",
        "LABEL.build_vocab(train_dataset)\n",
        "\n",
        "print(\"単語の件数の Top 10\")\n",
        "print(TEXT.vocab.freqs.most_common(10))\n",
        "\n",
        "print(\"\")\n",
        "print(\"ラベルごと件数\")\n",
        "print(LABEL.vocab.freqs)\n",
        "\n",
        "# テキストのバッチ化\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_dataset, val_dataset, test_dataset),\n",
        "    batch_size=32,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    repeat=False,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 単語数\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "# 埋め込みベクトル\n",
        "word_embeddings = TEXT.vocab.vectors "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                           \n",
            "100%|█████████▉| 399794/400000 [00:38<00:00, 10119.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "単語の件数の Top 10\n",
            "[('the', 225396), ('a', 111558), ('and', 110886), ('of', 101230), ('to', 93557), ('is', 73319), ('in', 63226), ('i', 49185), ('this', 48605), ('that', 46453)]\n",
            "\n",
            "ラベルごと件数\n",
            "Counter({'pos': 8752, 'neg': 8748})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGfR-oJkJg72",
        "outputId": "2a3f92a7-bd4f-4c7f-c39b-63a895bf0c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(vocab_size)\n",
        "print(TEXT.vocab.vectors.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55508\n",
            "torch.Size([55508, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5XLxf5DLA_Q",
        "outputId": "8623d783-a5c3-4b32-87f9-02c7a6465df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "# データの確認\n",
        "\n",
        "for i, batch in enumerate(train_iter):\n",
        "  print(\"# (batch_size, seq_length) -> fix_length=200 としたので、seq_length=200\")\n",
        "  print(batch.text[0].size())\n",
        "  print(\"\")\n",
        "  print(\"# 200 単語に満たない文の場合、<pad>, すなわち 1 で200単語になるまでパディングされる\")\n",
        "  print(batch.text[0][0])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ラベルのサイズ\")\n",
        "  print(batch.text[1].size())\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# 1 データ目の単語列（数字）\")\n",
        "  print(\"# text[0][1] testのバッチの、2サンプル目\")\n",
        "  print(batch.text[0][1])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# 1 データ目の単語（文字に逆変換）\")\n",
        "  print([TEXT.vocab.itos[data] for data in batch.text[0][1].tolist()])\n",
        "\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# (batch_size, seq_length) -> fix_length=200 としたので、seq_length=200\n",
            "torch.Size([32, 200])\n",
            "\n",
            "# 200 単語に満たない文の場合、<pad>, すなわち 1 で200単語になるまでパディングされる\n",
            "tensor([  207,    10,    29,     2, 24059,    25,  1980,   106,     2,   199,\n",
            "            4,    21,   355,   983,  1251,  3853,   494,     3, 20710, 21320,\n",
            "           13,   165,   236,    12,   260,     6,   245,    10,     7,     3,\n",
            "           84,    87,   199,     4,    35,  4513,     6,  4876, 14985,    16,\n",
            "          295,   301,   591,     3,  3823,     4,   161,    47,    19,     3,\n",
            "        20447,    16,     3,   277,    27, 50538,    13, 25446,  2819,     5,\n",
            "          604,     0,    12,   617,     2,    77,     5,     2,   182,   199,\n",
            "           19,     2,  1555,     5,     2,  4694,     8, 23650,   247,    82,\n",
            "          544,    10,     6,    28,     3,    20,    43,     2, 11665,    44,\n",
            "          955, 11265,     7,  1731, 25610,   100,  2446,     6,     2,  1015,\n",
            "          573,   115,     5,    25,   673,   941,    86,     0, 54362,     0,\n",
            "            4,    45,  9904,   158,    64,   211,    17,    47,   101,     0,\n",
            "           13,     0,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
            "\n",
            "# ラベルのサイズ\n",
            "torch.Size([32])\n",
            "\n",
            "# 1 データ目の単語列（数字）\n",
            "# text[0][1] testのバッチの、2サンプル目\n",
            "tensor([  126,   376,   180,     5,   276,    16, 49234,   130,     6,   884,\n",
            "           16,    57,  3039,   191,  1754,     3,     0,    16, 27644,     2,\n",
            "           62,  5156, 10005,  2805,    11,    22,    62,  1598,     6,   766,\n",
            "          138,  2106,   257,   126,     2,   762,     5,     0,     9,  3735,\n",
            "            2,  2524,    19,    10,    30,     4,     2,   414,  1079,   353,\n",
            "            2,    20,  2224,    90,  2199,   114,    53,   143,    11,     0,\n",
            "            4,  6034,   114,    27,    85, 27782,     4,    57,   704,   174,\n",
            "        11703,    34,  3119,   118,   157,   987,    40,    38,    15,   122,\n",
            "          114,     3,    62,    98,   330,     5,  7849,  1582,  1565,   132,\n",
            "        10643,     4,     2,   374,    87,   269,   966,    11,   240,   365,\n",
            "           59,  3154,    17,     3,  3600,     7,   296,     0, 41912,     6,\n",
            "            3,   153,   798,   555,   118,   138,    17,     3,  1070, 23466,\n",
            "        26773,     0,    14,   484,     4,  7055,   778,   459,     0,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
            "\n",
            "# 1 データ目の単語（文字に逆変換）\n",
            "['over', 'several', 'years', 'of', 'looking', 'for', 'half-decent', 'films', 'to', 'rent', 'for', 'my', 'kids,', \"i've\", 'developed', 'a', '<unk>', 'for', 'spotting', 'the', 'really', 'cheesy,', 'direct-to-video', 'efforts', 'that', 'are', 'really', 'painful', 'to', 'sit', 'through', '(for', 'anyone', 'over', 'the', 'age', 'of', '<unk>', 'i', 'dropped', 'the', 'ball', 'on', 'this', 'one', 'and', 'the', 'kids', 'spent', 'half', 'the', 'movie', 'asking', 'me', '\"what', 'did', 'she', 'say', 'that', '<unk>', 'and', '\"why', 'did', 'he', 'do', 'that?\"', 'and', 'my', 'eyes', 'got', 'sore', 'from', 'rolling', 'them', 'every', 'minute', 'or', 'so', 'as', 'characters', 'did', 'a', 'really', 'bad', 'job', 'of', 'introducing', 'seemingly', 'random', 'plot', 'changes.', 'and', 'the', 'next', 'time', 'someone', 'decides', 'that', 'having', 'absolutely', 'no', 'skill', 'with', 'a', 'sword', 'is', 'simply', '<unk>', 'realism\"', 'to', 'a', 'film,', 'please', 'run', 'them', 'through', 'with', 'a', 'dull', 'butter', 'knife.', '<unk>', 'was', 'head', 'and', 'shoulders', 'above', 'this.', '<unk>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0IR3bVjLmqV"
      },
      "source": [
        "class LstmClassifier(nn.Module):\n",
        "  def __init__(self, batch_size, hidden_size, output_size, vocab_size, embedding_length, weights):\n",
        "    super().__init__()\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_length)\n",
        "\n",
        "    # 学習済み埋め込みベクトルを使用\n",
        "    self.embed.weight.data.copy_(weights)\n",
        "    self.lstm = nn.LSTM(embedding_length, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embed(x)\n",
        "\n",
        "    h0 = torch.zeros(1, self.batch_size, self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(1, self.batch_size, self.hidden_size).to(device) \n",
        "\n",
        "    x, (h, c) = self.lstm(x, (h0, c0))\n",
        "\n",
        "    out = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "    return out \n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "\n",
        "net = LstmClassifier(batch_size, hidden_size, output_size, vocab_size, embedding_length, word_embeddings)\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWvfQslnScqj"
      },
      "source": [
        "# 学習ループ\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  val_loss = 0\n",
        "  val_acc = 0\n",
        "\n",
        "  net.train()\n",
        "  for i, batch in enumerate(train_iter):\n",
        "    text = batch.text[0].to(device)\n",
        "    #if (text.size()[0] is not 32):\n",
        "    #  continue\n",
        "    labels = batch.label.to(device)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    outputs = net(text)\n",
        "    loss = criterion(outputs, labels)\n",
        "    train_loss += loss.item()\n",
        "    train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBtXBKtTtWA",
        "outputId": "504130fb-40c0-4e7a-8b17-dbf6e4985984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# テストデータで推論\n",
        "\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  total = 0\n",
        "  test_acc = 0\n",
        "  for batch in test_iter:\n",
        "    text = batch.text[0].to(device)\n",
        "    if (text.size()[0] is not 32):\n",
        "      continue \n",
        "    labels = batch.label.to(device)\n",
        "\n",
        "    outputs = net(text)\n",
        "    test_acc +=  (outputs.max(1)[1] == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "\n",
        "  print(\"精度 : {} %\".format(100 * test_acc / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "精度 : 74.90396927016646 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}